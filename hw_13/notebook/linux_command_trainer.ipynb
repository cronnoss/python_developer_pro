{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linux Command Chatbot - Training Pipeline\n",
    "\n",
    "**Project**: Fine-tune TinyLlama for Linux command explanations\n",
    "\n",
    "**Dataset**: 101 Linux Commands by Bobby Iliev\n",
    "\n",
    "**Model**: TinyLlama-1.1B-Chat-v1.0 with LoRA adapters\n",
    "\n",
    "**Training Time**: ~30-45 minutes on Colab T4 GPU, 1-2 hours on MPS(Apple Silicon) or CPU\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Dependencies](#section-1)\n",
    "2. [Data Acquisition](#section-2)\n",
    "3. [HTML Parsing and Extraction](#section-3)\n",
    "4. [Data Cleaning](#section-4)\n",
    "5. [Dataset Creation and Augmentation](#section-5)\n",
    "6. [Load Base Model](#section-6)\n",
    "7. [Prepare Dataset for Training](#section-7)\n",
    "8. [Configure LoRA](#section-8)\n",
    "9. [Training Configuration](#section-9)\n",
    "10. [Fine-Tuning Execution (OPTIONAL)](#section-10)\n",
    "11. [Inference Testing](#section-11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-1'></a>\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "Install required packages and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(26722) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úì Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q transformers datasets peft trl bitsandbytes accelerate beautifulsoup4 requests\n",
    "\n",
    "print(\"‚úì Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully!\n",
      "\n",
      "============================================================\n",
      "Device Configuration:\n",
      "============================================================\n",
      "  Device: mps\n",
      "  Quantization (4-bit): ‚úó Disabled\n",
      "  Apple Silicon GPU (MPS) detected\n",
      "  ‚ö†Ô∏è bitsandbytes not supported - using float16\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")\n",
    "\n",
    "# ============================================================\n",
    "# Device Detection - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\n",
    "# ============================================================\n",
    "def get_device_config():\n",
    "    \"\"\"\n",
    "    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (device_name, use_quantization)\n",
    "        - CUDA: bitsandbytes handles 4-bit quantization in GPU memory\n",
    "        - MPS (Apple Silicon): no bitsandbytes support, use float16\n",
    "        - CPU: no quantization, use float16\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\", True\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return \"mps\", False\n",
    "    else:\n",
    "        return \"cpu\", False\n",
    "\n",
    "DEVICE, USE_QUANTIZATION = get_device_config()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Device Configuration:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Quantization (4-bit): {'‚úì Enabled' if USE_QUANTIZATION else '‚úó Disabled'}\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "elif DEVICE == \"mps\":\n",
    "    print(\"  Apple Silicon GPU (MPS) detected\")\n",
    "    print(\"  ‚ö†Ô∏è bitsandbytes not supported - using float16\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è Running on CPU - training will be VERY slow!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-2'></a>\n",
    "## 2. Data Acquisition\n",
    "\n",
    "Download the HTML file programmatically from GitHub releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 101 Linux Commands HTML...\n",
      "‚úì Downloaded 1020874 characters\n",
      "‚úì File saved: 101-linux-commands.html\n"
     ]
    }
   ],
   "source": [
    "# Download HTML file from GitHub releases\n",
    "url = \"https://github.com/bobbyiliev/101-linux-commands/releases/latest/download/101-linux-commands.html\"\n",
    "\n",
    "print(\"Downloading 101 Linux Commands HTML...\")\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "html_content = response.text\n",
    "\n",
    "# Save locally for reproducibility\n",
    "with open(\"101-linux-commands.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"‚úì Downloaded {len(html_content)} characters\")\n",
    "print(\"‚úì File saved: 101-linux-commands.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-3'></a>\n",
    "## 3. HTML Parsing and Data Extraction\n",
    "\n",
    "Parse HTML and extract command-description pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HTML Structure Inspection ===\n",
      "Total headings (h1-h3): 1656\n",
      "Total paragraphs: 1047\n",
      "\n",
      "First 5 headings:\n",
      "1. 101 Linux Commands\n",
      "2. Hacktoberfest\n",
      "3. About me\n",
      "4. DigitalOcean\n",
      "5. DevDojo\n"
     ]
    }
   ],
   "source": [
    "# Load HTML\n",
    "with open(\"101-linux-commands.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Inspect HTML structure first\n",
    "print(\"=== HTML Structure Inspection ===\")\n",
    "print(\"Total headings (h1-h3):\", len(soup.find_all(['h1', 'h2', 'h3'])))\n",
    "print(\"Total paragraphs:\", len(soup.find_all('p')))\n",
    "print(\"\\nFirst 5 headings:\")\n",
    "for i, heading in enumerate(soup.find_all(['h2', 'h3'])[:5]):\n",
    "    print(f\"{i+1}. {heading.get_text().strip()[:250]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total commands extracted: 160\n",
      "\n",
      "1. Command: ls\n",
      "   Description: The ls command lets you see the files and directories inside a specific directory (current working directory by default) .\n",
      "It normally lists the files and directories in ascending alphabetical order. ...\n",
      "   Examples: ['ls', 'ls {Directory_Path}', 'ls -lah']\n",
      "\n",
      "2. Command: cd\n",
      "   Description: The cd command is used to change the current working directory (i.e., the directory in which the current user is working) . The \"cd\" stands for \" c hange d irectory\" and it is one of the most frequent...\n",
      "   Examples: ['cd [OPTIONS] [directory]', 'cd /path/to/directory', 'cd ~']\n",
      "\n",
      "3. Command: cat\n",
      "   Description: The cat command allows us to create single or multiple files, to view the content of a file or to concatenate files and redirect the output to the terminal or files. The \"cat\" stands for 'concatenate....\n",
      "   Examples: ['cat <specified_file_name>', 'cat file1 file2 ...', 'cat > file_name']\n",
      "\n",
      "4. Command: tac\n",
      "   Description: tac is a Linux command that allows you to view files line-by-line, beginning from the last line. (tac doesn't reverse the contents of each individual line, only the order in which the lines are presen...\n",
      "   Examples: ['tac <specified_file_name>', 'tac -b concat_file_name tac_example_file_name', 'tac -r concat_file_name tac_example_file_name']\n",
      "\n",
      "5. Command: head\n",
      "   Description: The head command prints the first ten lines of a file. Use the -n option with a number (should be an integer) of lines to display. This command will display the first ten lines of the file foo.txt . E...\n",
      "   Examples: ['head filename.txt', 'head [OPTION] [FILENAME]', 'head -n 10 foo.txt']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "# Load HTML\n",
    "with open(\"101-linux-commands.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "commands_data = []\n",
    "\n",
    "# Find all h1 tags which denote command sections\n",
    "all_h1 = soup.find_all(\"h1\")\n",
    "\n",
    "def get_section_content(h1_tag):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å–µ–∫—Ü–∏–∏ –∫–æ–º–∞–Ω–¥—ã –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ h1 –∏–ª–∏ page-break\"\"\"\n",
    "    content_parts = []\n",
    "    examples = []\n",
    "    syntax = \"\"\n",
    "\n",
    "    # –ò–¥—ë–º –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º –ø–æ—Å–ª–µ h1\n",
    "    current = h1_tag.next_sibling\n",
    "    while current:\n",
    "        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º h1 –∏–ª–∏ page-break div\n",
    "        if hasattr(current, 'name'):\n",
    "            if current.name == 'h1':\n",
    "                break\n",
    "            if current.name == 'div' and 'page-break' in current.get('style', ''):\n",
    "                break\n",
    "\n",
    "            # –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –æ–ø–∏—Å–∞–Ω–∏—è (–¥–æ Examples –∏–ª–∏ Syntax)\n",
    "            if current.name == 'p':\n",
    "                text = current.get_text(\" \", strip=True)\n",
    "                if text and len(text) > 10:\n",
    "                    content_parts.append(text)\n",
    "\n",
    "            # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑ pre > code\n",
    "            if current.name == 'pre':\n",
    "                code = current.find('code')\n",
    "                if code:\n",
    "                    example_code = code.get_text().strip()\n",
    "                    if example_code and len(example_code) < 100:  # –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥\n",
    "                        examples.append(example_code)\n",
    "\n",
    "            # –ò—â–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –ø–æ—Å–ª–µ h3 \"Syntax\"\n",
    "            if current.name == 'h3':\n",
    "                h3_text = current.get_text().strip().lower()\n",
    "                if 'syntax' in h3_text:\n",
    "                    next_pre = current.find_next('pre')\n",
    "                    if next_pre:\n",
    "                        code = next_pre.find('code')\n",
    "                        if code:\n",
    "                            syntax = code.get_text().strip()\n",
    "\n",
    "        current = current.next_sibling\n",
    "\n",
    "    return content_parts, examples[:5], syntax  # –ú–∞–∫—Å–∏–º—É–º 5 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "\n",
    "for h1 in all_h1:\n",
    "    h1_text = h1.get_text().strip()\n",
    "    if not re.match(r\"^The\\s+.+\\s+[Cc]ommand$\", h1_text):\n",
    "        continue\n",
    "\n",
    "    code_tag = h1.find(\"code\")\n",
    "    if not code_tag:\n",
    "        continue\n",
    "\n",
    "    command_name = code_tag.get_text().strip()\n",
    "\n",
    "    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç\n",
    "    content_parts, examples, syntax = get_section_content(h1)\n",
    "\n",
    "    # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\n",
    "    description_parts = []\n",
    "\n",
    "    # –û—Å–Ω–æ–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–ø–µ—Ä–≤—ã–µ 2-3 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞)\n",
    "    if content_parts:\n",
    "        description_parts.extend(content_parts[:3])\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å\n",
    "    if syntax:\n",
    "        description_parts.append(f\"Syntax: {syntax}\")\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã\n",
    "    if examples:\n",
    "        examples_text = \"Examples: \" + \", \".join(examples[:3])\n",
    "        description_parts.append(examples_text)\n",
    "\n",
    "    full_description = \" \".join(description_parts)\n",
    "\n",
    "    if full_description:\n",
    "        commands_data.append({\n",
    "            \"command\": command_name,\n",
    "            \"description\": full_description,\n",
    "            \"examples\": examples,\n",
    "            \"syntax\": syntax,\n",
    "        })\n",
    "\n",
    "print(f\"Total commands extracted: {len(commands_data)}\")\n",
    "for i, cmd in enumerate(commands_data[:5]):\n",
    "    print(f\"\\n{i+1}. Command: {cmd['command']}\")\n",
    "    print(f\"   Description: {cmd['description'][:200]}...\")\n",
    "    if cmd['examples']:\n",
    "        print(f\"   Examples: {cmd['examples'][:3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-4'></a>\n",
    "## 4. Data Cleaning and Transformation\n",
    "\n",
    "Clean extracted text and prepare for training format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Cleaned commands: 159\n",
      "\n",
      "Example after cleaning:\n",
      "Command: ls\n",
      "Description: The ls command lets you see the files and directories inside a specific directory (current working directory by default) . It normally lists the files and directories in ascending alphabetical order. ...\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean extracted text for training\"\"\"\n",
    "    # Remove multiple spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\-:;()\\[\\]{}]', '', text)\n",
    "    \n",
    "    # Limit to reasonable length (~1000 chars max for richer descriptions)\n",
    "    if len(text) > 1000:\n",
    "        sentences = text.split('.')\n",
    "        # Keep first 6 sentences\n",
    "        text = '. '.join(sentences[:6])\n",
    "        if text and not text.endswith('.'):\n",
    "            text += '.'\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_command_name(command: str) -> str:\n",
    "    \"\"\"Extract clean command name\"\"\"\n",
    "    # Remove markdown symbols, numbers, etc.\n",
    "    command = re.sub(r'^\\d+\\.\\s*', '', command)  # Remove \"1. \"\n",
    "    command = re.sub(r'[#*`]', '', command)      # Remove markdown\n",
    "    command = command.strip()\n",
    "    \n",
    "    # Extract first word if it's a compound phrase\n",
    "    words = command.split()\n",
    "    if words:\n",
    "        return words[0].lower()\n",
    "    return command.lower()\n",
    "\n",
    "# Clean the data\n",
    "cleaned_data = []\n",
    "for item in commands_data:\n",
    "    cmd = clean_command_name(item['command'])\n",
    "    desc = clean_text(item['description'])\n",
    "    \n",
    "    # Skip if too short or too long\n",
    "    if len(desc) < 50 or len(cmd) < 2:\n",
    "        continue\n",
    "    \n",
    "    cleaned_data.append({\n",
    "        'command': cmd,\n",
    "        'description': desc\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Cleaned commands: {len(cleaned_data)}\")\n",
    "print(\"\\nExample after cleaning:\")\n",
    "print(f\"Command: {cleaned_data[0]['command']}\")\n",
    "print(f\"Description: {cleaned_data[0]['description'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-5'></a>\n",
    "## 5. Dataset Creation and Augmentation\n",
    "\n",
    "Convert to instruction-following format with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Total training examples: 795\n",
      "‚úì Augmentation ratio: 5.0x\n",
      "\n",
      "Example variations for one command:\n",
      "\n",
      "1. Instruction: Explain the Linux command ls\n",
      "   Output: The ls command lets you see the files and directories inside a specific director...\n",
      "\n",
      "2. Instruction: What does the ls command do?\n",
      "   Output: The ls command lets you see the files and directories inside a specific director...\n",
      "\n",
      "3. Instruction: How do I use ls in Linux?\n",
      "   Output: The ls command lets you see the files and directories inside a specific director...\n"
     ]
    }
   ],
   "source": [
    "# Question templates for augmentation\n",
    "question_templates = [\n",
    "    \"Explain the Linux command {cmd}\",\n",
    "    \"What does the {cmd} command do?\",\n",
    "    \"How do I use {cmd} in Linux?\",\n",
    "    \"What is the {cmd} command used for?\",\n",
    "    \"Describe the {cmd} command\",\n",
    "]\n",
    "\n",
    "# Create augmented dataset\n",
    "augmented_dataset = []\n",
    "\n",
    "for item in cleaned_data:\n",
    "    cmd = item['command']\n",
    "    desc = item['description']\n",
    "    \n",
    "    # Generate multiple training examples per command\n",
    "    for template in question_templates:\n",
    "        question = template.format(cmd=cmd)\n",
    "        \n",
    "        augmented_dataset.append({\n",
    "            'instruction': question,\n",
    "            'input': '',\n",
    "            'output': desc\n",
    "        })\n",
    "\n",
    "print(f\"‚úì Total training examples: {len(augmented_dataset)}\")\n",
    "print(f\"‚úì Augmentation ratio: {len(augmented_dataset) / len(cleaned_data):.1f}x\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample variations for one command:\")\n",
    "cmd_examples = [ex for ex in augmented_dataset if 'ls' in ex['instruction'].lower()][:3]\n",
    "for i, ex in enumerate(cmd_examples, 1):\n",
    "    print(f\"\\n{i}. Instruction: {ex['instruction']}\")\n",
    "    print(f\"   Output: {ex['output'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset saved to: data/linux_commands.jsonl\n",
      "‚úì File size: 409.95 KB\n"
     ]
    }
   ],
   "source": [
    "# Save to JSONL\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "with open('data/linux_commands.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in augmented_dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"‚úì Dataset saved to: data/linux_commands.jsonl\")\n",
    "\n",
    "# Verify file size\n",
    "file_size = os.path.getsize('data/linux_commands.jsonl') / 1024\n",
    "print(f\"‚úì File size: {file_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-6'></a>\n",
    "## 6. Load Base Model and Tokenizer\n",
    "\n",
    "Load TinyLlama with conditional configuration:\n",
    "- **CUDA (NVIDIA GPU)**: 4-bit quantization with bitsandbytes for memory efficiency\n",
    "- **MPS (Apple Silicon)**: float16 without quantization\n",
    "- **CPU**: float16 (will be slow)\n",
    "\n",
    "---\n",
    "### üìã Kaggle/Colab GPU Instructions\n",
    "\n",
    "**Kaggle:**\n",
    "1. Settings ‚Üí Accelerator ‚Üí **GPU T4 x2** or **GPU P100**\n",
    "2. Restart notebook\n",
    "\n",
    "**Google Colab:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n",
    "2. Restart runtime\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Device: mps, Quantization: False\n",
      "This may take a few minutes...\n",
      "\n",
      "‚úì Tokenizer loaded\n",
      "  Vocabulary size: 32000\n",
      "\n",
      "  Loading without quantization (float16) for mps...\n",
      "\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Device: {DEVICE}, Quantization: {USE_QUANTIZATION}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Recommended for training\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Load model based on device capabilities\n",
    "if USE_QUANTIZATION:\n",
    "    # CUDA: Use 4-bit quantization with bitsandbytes\n",
    "    print(\"\\n  Loading with 4-bit quantization (bitsandbytes)...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    # MPS/CPU: Load without quantization in float16\n",
    "    print(f\"\\n  Loading without quantization (float16) for {DEVICE}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    # Move model to device\n",
    "    if DEVICE == \"mps\":\n",
    "        model = model.to(\"mps\")\n",
    "    # For CPU, keep on CPU (default)\n",
    "\n",
    "print(\"\\n‚úì Model loaded successfully!\")\n",
    "print(f\"  Model device: {next(model.parameters()).device}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-7'></a>\n",
    "## 7. Prepare Dataset for Training\n",
    "\n",
    "Format dataset with proper chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 795 examples [00:00, 40594.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 795\n",
      "Features: {'instruction': Value('string'), 'input': Value('string'), 'output': Value('string')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 795/795 [00:00<00:00, 44715.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Formatted example:\n",
      "============================================================\n",
      "<|user|>\n",
      "Answer in English. Be concise and technical.\n",
      "User question: Explain the Linux command ls\n",
      "<|assistant|>\n",
      "The ls command lets you see the files and directories inside a specific directory (current working directory by default) . It normally lists the files and directories in ascending alphabetical order. In this interactive tutorial, you will learn the different ways to use the ls command: T\n",
      "...\n",
      "\n",
      "‚úì Train size: 715\n",
      "‚úì Eval size: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = Dataset.from_json('data/linux_commands.jsonl')\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "\n",
    "# Format with TinyLlama chat template\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format example with language enforcement\"\"\"\n",
    "    prompt = f\"\"\"<|user|>\n",
    "Answer in English. Be concise and technical.\n",
    "User question: {example['instruction']}\n",
    "<|assistant|>\n",
    "{example['output']}\"\"\"\n",
    "    return {'text': prompt}\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(format_instruction, remove_columns=['instruction', 'input', 'output'])\n",
    "\n",
    "# Show example\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Formatted example:\")\n",
    "print(\"=\"*60)\n",
    "print(dataset[0]['text'][:400])\n",
    "print(\"...\")\n",
    "\n",
    "# Split into train/eval\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"\\n‚úì Train size: {len(dataset['train'])}\")\n",
    "print(f\"‚úì Eval size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-8'></a>\n",
    "## 8. Configure LoRA\n",
    "\n",
    "Set up PEFT with LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for mps training...\n",
      "\n",
      "============================================================\n",
      "LoRA Configuration:\n",
      "============================================================\n",
      "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for training\n",
    "if USE_QUANTIZATION:\n",
    "    # Only needed for quantized models\n",
    "    print(\"Preparing model for k-bit training...\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "else:\n",
    "    # For MPS/CPU: enable gradient checkpointing to save memory\n",
    "    print(f\"Preparing model for {DEVICE} training...\")\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # LoRA rank\n",
    "    lora_alpha=32,           # Scaling factor\n",
    "    target_modules=[         # Target attention modules\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LoRA Configuration:\")\n",
    "print(\"=\"*60)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-9'></a>\n",
    "## 9. Training Configuration\n",
    "\n",
    "Configure trainer and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: adamw_torch\n",
      "FP16: False, BF16: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 715/715 [00:00<00:00, 29912.70 examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 715/715 [00:00<00:00, 6027.66 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 715/715 [00:00<00:00, 26070.15 examples/s]\n",
      "Adding EOS to eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:00<00:00, 22765.75 examples/s]\n",
      "Tokenizing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:00<00:00, 5080.62 examples/s]\n",
      "Truncating eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:00<00:00, 56054.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Trainer configured successfully!\n",
      "\n",
      "Training configuration:\n",
      "  Device: mps\n",
      "  Epochs: 3\n",
      "  Batch size: 2\n",
      "  Gradient accumulation: 8\n",
      "  Effective batch size: 16\n",
      "  Learning rate: 0.0002\n",
      "  Optimizer: adamw_torch\n"
     ]
    }
   ],
   "source": [
    "# Training arguments - use SFTConfig instead of TrainingArguments\n",
    "from trl import SFTConfig\n",
    "\n",
    "# Device-specific optimizer and settings\n",
    "if USE_QUANTIZATION:  # CUDA\n",
    "    optimizer_name = \"paged_adamw_8bit\"  # Memory-efficient, requires bitsandbytes\n",
    "    use_fp16 = True\n",
    "    use_bf16 = False\n",
    "elif DEVICE == \"mps\":\n",
    "    optimizer_name = \"adamw_torch\"  # Standard PyTorch optimizer\n",
    "    use_fp16 = False  # MPS doesn't support fp16 training well\n",
    "    use_bf16 = False  # MPS M1/M2 doesn't support bf16\n",
    "else:  # CPU\n",
    "    optimizer_name = \"adamw_torch\"\n",
    "    use_fp16 = False\n",
    "    use_bf16 = False\n",
    "\n",
    "print(f\"Optimizer: {optimizer_name}\")\n",
    "print(f\"FP16: {use_fp16}, BF16: {use_bf16}\")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2 if DEVICE != \"cuda\" else 4,  # Smaller batch for MPS/CPU\n",
    "    gradient_accumulation_steps=8 if DEVICE != \"cuda\" else 4,  # Compensate smaller batch\n",
    "    learning_rate=2e-4,\n",
    "    fp16=use_fp16,\n",
    "    bf16=use_bf16,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=optimizer_name,\n",
    "    report_to=\"none\",               # Disable wandb\n",
    "    dataloader_pin_memory=False if DEVICE == \"mps\" else True,  # MPS doesn't support pin_memory\n",
    "    # SFT-specific parameters\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Trainer configured successfully!\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Optimizer: {optimizer_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "fine_tuning"
    ]
   },
   "source": [
    "<a id='section-10'></a>\n",
    "## 10. Fine-Tuning Execution (OPTIONAL)\n",
    "\n",
    "‚ö†Ô∏è **OPTIONAL CELL**: Skip this if using pre-trained adapters\n",
    "\n",
    "Training takes approximately **30-45 minutes** on Colab T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting training...\n",
      "This will take approximately 30-45 minutes.\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 24:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.763400</td>\n",
       "      <td>1.667363</td>\n",
       "      <td>1.677441</td>\n",
       "      <td>104868.000000</td>\n",
       "      <td>0.631944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.374200</td>\n",
       "      <td>1.367578</td>\n",
       "      <td>1.400391</td>\n",
       "      <td>209736.000000</td>\n",
       "      <td>0.680658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.123100</td>\n",
       "      <td>1.275015</td>\n",
       "      <td>1.312695</td>\n",
       "      <td>314604.000000</td>\n",
       "      <td>0.700126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úì Training completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è OPTIONAL CELL: Skip this if using pre-trained adapters\n",
    "# Training takes approximately 30-45 minutes on Colab T4 GPU\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(\"This will take approximately 30-45 minutes.\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Training completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LoRA adapters saved to: model/lora_adapters/\n",
      "‚úì Adapter size: 17.21 MB\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapters\n",
    "os.makedirs('model/lora_adapters', exist_ok=True)\n",
    "\n",
    "model.save_pretrained(\"model/lora_adapters\")\n",
    "tokenizer.save_pretrained(\"model/lora_adapters\")\n",
    "\n",
    "print(\"‚úì LoRA adapters saved to: model/lora_adapters/\")\n",
    "\n",
    "# Check adapter size\n",
    "adapter_path = \"model/lora_adapters/adapter_model.safetensors\"\n",
    "if os.path.exists(adapter_path):\n",
    "    size_mb = os.path.getsize(adapter_path) / (1024 * 1024)\n",
    "    print(f\"‚úì Adapter size: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Adapter file not found. Check for adapter_model.bin instead.\")\n",
    "    adapter_path_alt = \"model/lora_adapters/adapter_model.bin\"\n",
    "    if os.path.exists(adapter_path_alt):\n",
    "        size_mb = os.path.getsize(adapter_path_alt) / (1024 * 1024)\n",
    "        print(f\"‚úì Adapter size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-11'></a>\n",
    "## 11. Inference Testing\n",
    "\n",
    "Test the fine-tuned model with various questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MPS memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Clear memory - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –≤—Å–µ—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤\n",
    "if 'trainer' in globals():\n",
    "    del trainer\n",
    "gc.collect()\n",
    "\n",
    "# Device-specific cache clearing\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úì CUDA memory cleared\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"‚úì MPS memory cleared\")\n",
    "else:\n",
    "    print(\"‚úì CPU memory cleared (gc.collect)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model for inference on mps...\n",
      "Loading LoRA adapters...\n",
      "\n",
      "‚úì Model ready for inference on mps!\n"
     ]
    }
   ],
   "source": [
    "# Load base model for inference\n",
    "print(f\"Loading base model for inference on {DEVICE}...\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "else:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    if DEVICE == \"mps\":\n",
    "        base_model = base_model.to(\"mps\")\n",
    "\n",
    "# Load LoRA adapters\n",
    "print(\"Loading LoRA adapters...\")\n",
    "model = PeftModel.from_pretrained(base_model, \"model/lora_adapters\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model/lora_adapters\")\n",
    "\n",
    "print(f\"\\n‚úì Model ready for inference on {DEVICE}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Inference function defined\n"
     ]
    }
   ],
   "source": [
    "# Inference function\n",
    "def ask_bot(question: str, max_tokens: int = 150) -> str:\n",
    "    \"\"\"Ask the bot a question\"\"\"\n",
    "    prompt = f\"\"\"<|user|>\n",
    "Answer in English. Be concise and technical.\n",
    "User question: {question}\n",
    "<|assistant|>\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only assistant's response\n",
    "    if \"<|assistant|>\" in response:\n",
    "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úì Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INFERENCE TESTS\n",
      "============================================================\n",
      "\n",
      "[Test 1]\n",
      "Question: Explain the ls command\n",
      "Answer: The ls command is a common command in Unix/Linux that displays the list of files and directories in a directory hierarchy. It works by listing the contents of a specific directory or file system path using its syntax:\n",
      "\n",
      "```\n",
      "ls [OPTION]... [PATH]\n",
      "```\n",
      "\n",
      "Here are some examples of how to use this command:\n",
      "\n",
      "1. List all the files in a specific directory:\n",
      "   ```\n",
      "   ls -a\n",
      "   ```\n",
      "\n",
      "   This will show both regular files (i.e., those with a dot extension (.txt, .png, etc.) and symbolic links (i.e., which point to other locations).\n",
      "\n",
      "2. List only the files with a certain extension:\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 2]\n",
      "Question: What does grep do?\n",
      "Answer: Grep is a command-line utility used for searching text files for specific patterns or strings. It operates on the line by line basis and searches for any occurrence of specified pattern(s) in each line. Here's how it works:\n",
      "\n",
      "1. The first argument to the `grep` command specifies the pattern to search for, either as a single string or as a regular expression. For example:\n",
      "\n",
      "```bash\n",
      "$ echo \"Hello World\" | grep -E 'h[e|l]l o'\n",
      "Hello World\n",
      "```\n",
      "\n",
      "This command will search for any occurrence of \"hello\" in the input string. 2. The second argument is optional and defines the directory in which to search\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 3]\n",
      "Question: How to use chmod?\n",
      "Answer: Chmod is a command-line utility used for changing the access permissions of files or directories on Unix-like systems. It works by specifying the mode (octal value) for each file or directory's permissions. Here's an example of how to use chmod to change the permissions of a specific file, `myfile`:\n",
      "\n",
      "```bash\n",
      "chmod 400 myfile\n",
      "```\n",
      "\n",
      "In this example, we specify the octal value of `400` as the mode for `myfile`. This means that the owner (`uid`) has read, write, and execute permissions (`gid`) and no permission for others (`other`). To set the permissions for all users, we could use the\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 4]\n",
      "Question: –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –∫–æ–º–∞–Ω–¥–∞ cd?\n",
      "Answer: Command (or command line) `cd` stands for \"change directory\" or \"change to current directory\". It is a common command used when switching between different directories on your computer. Here's what it does:\n",
      "\n",
      "1. The `cd` command takes no arguments, so there is no argument passed to it.\n",
      "2. When you run the command, the shell prompt changes to reflect that you are currently changing directories. For example, if you run `cd`, the prompt will show \"cd > \".\n",
      "3. Once you have entered a new directory using `cd`, the shell will continue running with the new directory as its current working directory.\n",
      "4. To return back to the previous directory, use the `cd\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 5]\n",
      "Question: –û–ø–∏—à–∏ –∫–æ–º–∞–Ω–¥—É mkdir\n",
      "Answer: To create a new directory using the command line interface (CLI) on your system, you can use the `mkdir` command, which stands for \"make directory\". Here's an example of how to use this command:\n",
      "\n",
      "1. Navigate to the directory where you want to create the new directory:\n",
      "   ```bash\n",
      "   $ cd /path/to/directory\n",
      "   ```\n",
      "\n",
      "2. Run the `mkdir` command:\n",
      "   ```bash\n",
      "   $ mkdir filename\n",
      "   ```\n",
      "\n",
      "3. Replace `filename` with the name of the directory that you want to create:\n",
      "   ```bash\n",
      "   $ mkdir new_folder\n",
      "   ```\n",
      "\n",
      "4. Wait for the directory to\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test examples\n",
    "test_questions = [\n",
    "    \"Explain the ls command\",\n",
    "    \"What does grep do?\",\n",
    "    \"How to use chmod?\",\n",
    "    \"–ß—Ç–æ –¥–µ–ª–∞–µ—Ç –∫–æ–º–∞–Ω–¥–∞ cd?\",  # Russian: \"What does cd command do?\"\n",
    "    \"–û–ø–∏—à–∏ –∫–æ–º–∞–Ω–¥—É mkdir\",      # Russian: \"Describe mkdir command\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[Test {i}]\")\n",
    "    print(f\"Question: {question}\")\n",
    "    answer = ask_bot(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
