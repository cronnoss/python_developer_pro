# Toxicity Analysis API

FastAPI приложение для анализа токсичности текста на русском языке. Использует предобученную модель `cointegrated/rubert-tiny-toxicity` для классификации текста по категориям токсичности.

## Описание проекта

Этот проект представляет собой REST API сервис, который:
- Анализирует токсичность текста на русском языке
- Возвращает метку токсичности и оценку уверенности
- Использует модель на основе BERT для классификации

### Модель

Проект использует модель `cointegrated/rubert-tiny-toxicity` - это fine-tuned версия модели `cointegrated/rubert-tiny` для классификации токсичности и неуместности коротких неформальных русских текстов.
https://huggingface.co/cointegrated/rubert-tiny-toxicity

Модель классифицирует текст по следующим категориям:
- `non-toxic` - текст не содержит оскорблений, нецензурной лексики и угроз
- `insult` - оскорбления
- `obscenity` - нецензурная лексика
- `threat` - угрозы
- `dangerous` - неуместный контент, который может навредить репутации

## Структура проекта

```
hw_09/
├── app/
│   ├── __init__.py
│   └── app.py              # FastAPI приложение
├── model/
│   ├── __init__.py
│   ├── model.py            # Логика загрузки и использования модели
│   ├── setup.yaml          # Конфигурация модели
│   ├── README.md           # Документация модели
│   └── ...                 # Файлы модели (tokenizer, weights)
├── Dockerfile              # Docker конфигурация
├── requirements.txt        # Зависимости Python
└── README.md              # Эта документация
```

## Установка и запуск

### Предварительные требования

- Python 3.8+
- pip
- Docker (опционально)

### Локальная установка

1. **Клонируйте репозиторий:**
   ```bash
   git clone <repository-url>
   cd hw_09
   ```

2. **Установите зависимости:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Запустите приложение:**
   ```bash
   uvicorn app.app:app --host 127.0.0.1 --port 8080
   ```

   Приложение будет доступно по адресу: `http://127.0.0.1:8080`

### Запуск через Docker

1. **Соберите Docker образ:**
   ```bash
   docker build -t hw09 .
   ```

2. **Запустите контейнер:**
   ```bash
   docker run -p 80:80 hw09
   ```

   Приложение будет доступно по адресу: `http://localhost:80`

## API Endpoints

### 1. Главная страница

**GET** `/`

Возвращает информацию о сервисе.

**Пример запроса:**
```bash
curl http://127.0.0.1:8080/
```

**Ответ:**
```json
{
  "text": "Toxicity Analysis"
}
```

### 2. Анализ токсичности

**GET** `/predict?text={текст}`

Анализирует токсичность переданного текста.

**Параметры:**
- `text` (string, обязательный) - текст для анализа

**Пример запроса:**
```bash
curl --get --data-urlencode "text=Это очень хороший текст" "http://127.0.0.1:80/predict"
```

**Ответ:**
```json
{
  "text": "Это очень хороший текст",
  "sentiment_label": "non-toxic",
  "sentiment_score": 0.9998
}
```

## Использование с Postman

### 1. Настройка Postman

1. Откройте Postman
2. Создайте новую коллекцию для проекта
3. Установите базовый URL: `http://127.0.0.1:8080` (или `http://localhost:80` для Docker)

### 2. Тестирование endpoints

#### GET / (Главная страница)

1. Создайте новый запрос
2. Выберите метод **GET**
3. Введите URL: `{{base_url}}/`
4. Нажмите **Send**

**Ожидаемый ответ:**
```json
{
  "text": "Toxicity Analysis"
}
```

#### GET /predict (Анализ токсичности)

1. Создайте новый запрос
2. Выберите метод **GET**
3. Введите URL: `{{base_url}}/predict`
4. Перейдите на вкладку **Params**
5. Добавьте параметр:
   - Key: `text`
   - Value: `Это тестовый текст для анализа токсичности`
6. Нажмите **Send**

**Ожидаемый ответ:**
```json
{
  "text": "Это тестовый текст для анализа токсичности",
  "sentiment_label": "non-toxic",
  "sentiment_score": 0.9997
}
```

### 3. Примеры тестовых запросов

#### Нетоксичный текст
```
GET {{base_url}}/predict?text=Привет,%20как%20дела?
```

#### Потенциально токсичный текст
```
GET {{base_url}}/predict?text=Иди%20ты%20нафиг!
```

#### Оскорбление
```
GET {{base_url}}/predict?text=Ты%20полный%20идиот
```

## Структура ответа

Все ответы от API имеют следующую структуру:

```json
{
  "text": "исходный текст",
  "sentiment_label": "метка токсичности",
  "sentiment_score": 0.9876
}
```

### Поля ответа:

- `text` (string) - исходный текст, переданный для анализа
- `sentiment_label` (string) - метка токсичности:
  - `non-toxic` - нетоксичный текст
  - `insult` - оскорбление
  - `obscenity` - нецензурная лексика
  - `threat` - угроза
  - `dangerous` - неуместный контент
- `sentiment_score` (float) - оценка уверенности модели (0.0 - 1.0)

## Обработка ошибок

API возвращает стандартные HTTP коды состояния:

- `200 OK` - успешный запрос
- `404 Not Found` - некорректные параметры запроса
- `500 Internal Server Error` - внутренняя ошибка сервера

## Разработка

### Структура кода

#### app/app.py
Основной файл FastAPI приложения:
- Определяет endpoints
- Загружает модель при запуске
- Обрабатывает запросы

#### model/model.py
Логика работы с моделью:
- Загружает предобученную модель
- Выполняет предсказания
- Возвращает структурированные результаты

#### model/setup.yaml
Конфигурация модели:
- Указывает задачу (`sentiment-analysis`)
- Указывает модель (`cointegrated/rubert-tiny-toxicity`)

### Добавление новых endpoints

Для добавления новых endpoints отредактируйте файл `app/app.py`:

```python
@app.get("/new-endpoint")
def new_endpoint():
    return {"message": "New endpoint"}
```

### Изменение модели

Для изменения модели отредактируйте файл `model/setup.yaml`:

```yaml
task: sentiment-analysis
model: your-new-model-name
```

## Мониторинг и логирование

Приложение использует стандартное логирование FastAPI. Для включения подробных логов запустите с флагом `--log-level debug`:

```bash
uvicorn app.app:app --host 127.0.0.1 --port 8080 --log-level debug
```

## Производительность

- Модель загружается один раз при запуске приложения
- Предсказания выполняются быстро благодаря использованию предобученной модели
- API поддерживает параллельные запросы

## Безопасность

- API не сохраняет переданные тексты
- Все данные обрабатываются в памяти
- Рекомендуется использовать HTTPS в продакшене

## Поддержка

При возникновении проблем:
1. Проверьте логи приложения
2. Убедитесь, что все зависимости установлены
3. Проверьте, что модель загружена корректно
4. Обратитесь к документации FastAPI и transformers 